---
title: "DATA624 - Homework 5A"
author: "Esteban Aramayo"
date: "2022-06-28"
output: openintro::lab_report
#output: word_document
#output: html
---

```{r global-options, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE,
                      message=FALSE,
                      collapse = FALSE,
                      comment = "#>" )
```


# Week 5: Nonlinear Regression Models, Regression Trees and Rules-Based Models [27-Jun 3-Jul]
 

## Week 5A: Nonlinear Regression Models
 

The exercises are from the textbook "Applied Predictive Modeling" by KJ


### Exercise 7.2


`Friedman (1991)` introduced several benchmark data sets create by simulation. 

One of these simulations used the following nonlinear equation to create data:

$y = 10 sin(\pi x_1 x_) + 20(x_3 − 0.5)^2 + 10x_4 + 5x_5 + N(0, \sigma^2)$

where the x values are random variables uniformly distributed between [0, 1] (there are also 5 other non-informative variables also created in the simulation).

The package `mlbench` contains a function called `mlbench.friedman1` that simulates these data:

```{r}
library(caret)
library(mlbench)

set.seed(200)
trainingData <- mlbench.friedman1(200, sd = 1)
## We convert the 'x' data from a matrix to a data frame
## One reason is that this will give the columns names.
trainingData$x <- data.frame(trainingData$x)
## Look at the data using
caret::featurePlot(trainingData$x, trainingData$y)
## or other methods.

## This creates a list with a vector 'y' and a matrix
## of predictors 'x'. Also simulate a large test set to
## estimate the true error rate with good precision:
testData <- mlbench.friedman1(5000, sd = 1)
testData$x <- data.frame(testData$x)


```
  
Tune several models on these data. For example:

```{r}

library(caret)
knnModel <- train(x = trainingData$x,
                  y = trainingData$y,
                  method = "knn",
                  preProc = c("center", "scale"),
                  tuneLength = 10)

knnModel

```


```{r}
knnPred <- predict(knnModel, newdata = testData$x)
## The function 'postResample' can be used to get the test set
## perforamnce values
postResample(pred = knnPred, obs = testData$y)
```

**Which models appear to give the best performance? Does MARS select the informative predictors (those named X1–X5)?**

<br>

#### Using a Neural Network model

First, we remove predictors to ensure that the maximum absolute pairwise correlation between the predictors is less than 0.75.

```{r}

tooHigh <- findCorrelation(cor(trainingData$x), cutoff = .75)

tooHigh

```
Since the correlation vector is empty, there are no predictors to remove. We can now fit a neural network model using the existing training set.

```{r}

# Resource: Chapter 7.5 of textbook "Applied Predictive Modeling" by KJ

## Create a specific candidate set of models to evaluate:
nnetGrid <- expand.grid(.decay = c(0, 0.01, .1),
                        .size = c(1:10),
                        .bag = FALSE)

# We can use caret to perform a grid search using 10-fold cross-validation.
# The train function generates a resampling estimate of performance. Because
# the training set size is not small, 10-fold cross-validation should produce
# reasonable estimates of model performance. The function trainControl specifies
# the type of resampling:
ctrl <- trainControl(method = "cv", number = 10)

nnetTune <- train(trainingData$x, trainingData$y,
                  method = "avNNet",
                  tuneGrid = nnetGrid,
                  trControl = ctrl,
                  ## Automatically standardize data prior to modeling
                  ## and prediction
                  preProc = c("center", "scale"),
                  linout = TRUE,
                  trace = FALSE,
                  MaxNWts = 10 * (ncol(trainingData$x) + 1) + 10 + 1,
                  maxit = 500)

nnetTune
```

Predict using the test data set and get the model's performance values

```{r}
nnetPred <- predict(nnetTune, newdata = testData$x)
## The function 'postResample' can be used to get the test set
## performance values
postResample(pred = nnetPred, obs = testData$y)
```

<br>

#### Using a Multivariate Adaptive Regression Splines (MARS) model

```{r}

# Resource: Chapter 7.5 of textbook "Applied Predictive Modeling" by KJ
# Resource: http://uc-r.github.io/mars

library(earth)
library(dplyr)

## Create a specific candidate set of models to evaluate:
marsGrid <- expand.grid(degree = 1:3,
                        nprune = seq(2, 100, length.out = 10) %>% floor()
  )

# We can use caret to perform a grid search using 10-fold cross-validation.
# The train function generates a resampling estimate of performance. Because
# the training set size is not small, 10-fold cross-validation should produce
# reasonable estimates of model performance. The function trainControl specifies
# the type of resampling:
ctrl <- trainControl(method = "cv", number = 10)


# cross validated model
tuned_mars <- train(
  x = trainingData$x,
  y = trainingData$y,
  method = "earth",
  metric = "RMSE",
  trControl = ctrl,
  tuneGrid = marsGrid
)

tuned_mars

```

Plot the MARS model results

```{r}
ggplot(tuned_mars)
```

Show the best tuned MARS model

```{r}
# best chosen MARS model
tuned_mars$bestTune
```

Predict using the test data set and get the model's performance values

```{r}
marsPred <- predict(tuned_mars, newdata = testData$x)
## The function 'postResample' can be used to get the test set
## performance values
postResample(pred = marsPred, obs = testData$y)
```

<br>

#### Using a Support Vector Machines (SVM) model

```{r}

# Resource: Chapter 7.5 of textbook "Applied Predictive Modeling" by KJ

library(kernlab)


# We can use caret to perform a grid search using 10-fold cross-validation.
# The train function generates a resampling estimate of performance. Because
# the training set size is not small, 10-fold cross-validation should produce
# reasonable estimates of model performance. The function trainControl specifies
# the type of resampling:
ctrl <- trainControl(method = "cv", number = 10)


svmRTuned <- train(trainingData$x, trainingData$y,
                   method = "svmRadial",
                   preProc = c("center", "scale"),
                   tuneLength = 14,
                   trControl = ctrl)

svmRTuned

```


Predict using the test data set and get the model's performance values

```{r}
svmPred <- predict(svmRTuned, newdata = testData$x)
## The function 'postResample' can be used to get the test set
## performance values
postResample(pred = svmPred, obs = testData$y)
```



**Which models appear to give the best performance? Does MARS select the informative predictors (those named X1–X5)?**

Comparing all used models we can see that the **MARS** model yields the best performance in terms of the lowest **RMSE = 1.2803060**.


Model                   |      RMSE  |  Rsquared |       MAE
------------------------|------------|-----------|----------
KNN                     |  3.2040595 | 0.6819919 | 2.5683461 
Neural Network (avNNet) |  2.1930855 | 0.8113903 | 1.6402075
MARS                    |  **1.2803060** | 0.9335241 | 1.0168673
SVM                     |  2.0793977 | 0.8249453 | 1.5796370 



<br>
<br>
<br>


### Exercise 7.5

Exercise 6.3 describes data for a chemical manufacturing process. Use the same data imputation, data splitting, and pre-processing steps as before and train several nonlinear regression models.


#### Solution

Load libraries used for Exercise 6.3

```{r}
library(RANN)
library(dplyr)
library(naniar)
library(caret)
```

#### Load the data

```{r}
library(AppliedPredictiveModeling)
data(ChemicalManufacturingProcess)
df_raw <- ChemicalManufacturingProcess
```

#### Perform imputation of missing values

```{r}
# check variables for missing values
# gg_miss_var(df_raw)
```

```{r}
preProcess_impute <- preProcess(df_raw, method ="knnImpute")
df <- predict(preProcess_impute, newdata = df_raw)
```

```{r}
# check variables for missing values
# gg_miss_var(df)
```

#### Remove near zero variance predictors

Predictors that have one unique value (i.e. are zero variance predictors) or predictors that are have both of the following characteristics: they have very few unique values relative to the number of samples and the ratio of the frequency of the most common value to the frequency of the second most common value is large. 

```{r}

df <- df %>% select(!caret::nearZeroVar(.))

```



#### Splitting data in to training and test set

```{r}
# Create the training and test datasets
set.seed(100)
# Step 1: Get row numbers for the training data
trainRowNumbers <- createDataPartition(df$Yield, p=0.8, list=FALSE)
# Step 2: Create the training  dataset
trainData <- df[trainRowNumbers,]
# Step 3: Create the test dataset
testData <- df[-trainRowNumbers,]
```

#### Data preprocess applying centering and scaling

```{r}
preProcValues    <- preProcess(trainData, method = c("center", "scale"))
trainTransformed <- predict(preProcValues, trainData)
testTransformed  <- predict(preProcValues, testData)

xTrain <- trainTransformed %>% select(-Yield)
yTrain <- trainTransformed %>% select(Yield)

xTest  <- testTransformed %>% select(-Yield)
yTest  <- testTransformed %>% select(Yield)
```

<br>

#### Fit a KNN model


```{r knn-chem}
knnChemModel <- train(x = xTrain,
                  y = yTrain$Yield,
                  method = "knn",
                  preProc = c("center", "scale"),
                  tuneLength = 10)

knnChemModel

```


```{r}
knnChemPred <- predict(knnChemModel, newdata = xTest)
## The function 'postResample' can be used to get the test set
## perforamnce values
postResample(pred = knnChemPred, obs = yTest$Yield)
```


<br>

#### Fit a Neural Network model

First, we remove predictors to ensure that the maximum absolute pairwise correlation between the predictors is less than 0.75.

```{r}

tooHigh <- findCorrelation(cor(xTrain), cutoff = .75)

tooHigh

```
Since the correlation vector is not empty, there are predictors to be removed. After removal, generate a new training and test sets of predictors. 

```{r}
trainXnnet <- xTrain[, -tooHigh]
testXnnet  <- xTest[, -tooHigh]
```


We can now fit a neural network model using the existing training set.

```{r nnet-chem}

# Resource: Chapter 7.5 of textbook "Applied Predictive Modeling" by KJ

## Create a specific candidate set of models to evaluate:
nnetGrid <- expand.grid(.decay = c(0, 0.01, .1),
                        .size = c(1:10),
                        .bag = FALSE)

# We can use caret to perform a grid search using 10-fold cross-validation.
# The train function generates a resampling estimate of performance. Because
# the training set size is not small, 10-fold cross-validation should produce
# reasonable estimates of model performance. The function trainControl specifies
# the type of resampling:
ctrl <- trainControl(method = "cv", number = 10)

nnetChemTune <- train(xTrain, yTrain$Yield,
                  method = "avNNet",
                  tuneGrid = nnetGrid,
                  trControl = ctrl,
                  ## Automatically standardize data prior to modeling
                  ## and prediction
                  preProc = c("center", "scale"),
                  linout = TRUE,
                  trace = FALSE,
                  MaxNWts = 10 * (ncol(trainXnnet) + 1) + 10 + 1,
                  maxit = 500)

nnetChemTune
```

Predict using the test data set and get the model's performance values

```{r}
nnetChemPred <- predict(nnetChemTune, newdata = xTest)
## The function 'postResample' can be used to get the test set
## performance values
postResample(pred = nnetChemPred, obs = yTest$Yield)
```
<br>

#### Fit a MARS model


```{r mars-chem}

# Resource: Chapter 7.5 of textbook "Applied Predictive Modeling" by KJ
# Resource: http://uc-r.github.io/mars

library(earth)
library(dplyr)

## Create a specific candidate set of models to evaluate:
marsGrid <- expand.grid(degree = 1:3,
                        nprune = seq(2, 100, length.out = 10) %>% floor()
  )

# We can use caret to perform a grid search using 10-fold cross-validation.
# The train function generates a resampling estimate of performance. Because
# the training set size is not small, 10-fold cross-validation should produce
# reasonable estimates of model performance. The function trainControl specifies
# the type of resampling:
ctrl <- trainControl(method = "cv", number = 10)


# cross validated model
tuned_ChemMARS <- train(
  x = xTrain,
  y = yTrain$Yield,
  method = "earth",
  metric = "RMSE",
  trControl = ctrl,
  tuneGrid = marsGrid
)

tuned_ChemMARS

```

Predict using the test data set and get the model's performance values

```{r}
marsChemPred <- predict(tuned_ChemMARS, newdata = xTest)
## The function 'postResample' can be used to get the test set
## performance values
postResample(pred = marsChemPred, obs = yTest$Yield)
```

<br>

#### Fit an SVM model


```{r svm-chem}

# Resource: Chapter 7.5 of textbook "Applied Predictive Modeling" by KJ

library(kernlab)


# We can use caret to perform a grid search using 10-fold cross-validation.
# The train function generates a resampling estimate of performance. Because
# the training set size is not small, 10-fold cross-validation should produce
# reasonable estimates of model performance. The function trainControl specifies
# the type of resampling:
ctrl <- trainControl(method = "cv", number = 10)


svmChemRTuned <- train(xTrain, yTrain$Yield,
                   method = "svmRadial",
                   preProc = c("center", "scale"),
                   tuneLength = 14,
                   trControl = ctrl)

svmChemRTuned

```


Predict using the test data set and get the model's performance values

```{r}
svmChemPred <- predict(svmChemRTuned, newdata = xTest)
## The function 'postResample' can be used to get the test set
## performance values
postResample(pred = svmChemPred, obs = yTest$Yield)
```



<br>

#### (a) Which nonlinear regression model gives the optimal resampling and test set performance?

Comparing all used models we can see that the **Neural Network (avNNet)** model yields the best performance in terms of the lowest RMSE = 0.5594510.


Model                   |      RMSE  |  Rsquared |       MAE
------------------------|------------|-----------|----------
KNN                     |  0.6501442 | 0.4623051 | 0.5205731 
Neural Network (avNNet) |  **0.5594510** | 0.6676931 | 0.4589402
MARS                    |  0.5754156 | 0.6002260 | 0.4630585
SVM                     |  0.5682649 | 0.5880968 | 0.4506714 


<br>

#### (b) Which predictors are most important in the optimal nonlinear regression model? Do either the biological or process variables dominate the list? How do the top ten important predictors compare to the top ten predictors from the optimal linear model?

* The **Neural Network (avNNet)** model yields the best performance in terms of the lowest RMSE = 0.5594510. For such model, below are the predictors in order of importance. A plot of them is also provided.

* The comparison of the top ten most important predictor between the NNET model and the Linear Model shows that the most important predictor is the `"ManufacturingProcess32"`. The rest of the predictors are ranked differently between the 2 models. The NNET model captured only one biological predictor `"BiologicalMaterial11"` in the top 10 list. While the Linear Model ranked 4 biological predictors in the top 10.

Importance|Linear Model Top Predictors| Overall|NNET Model Predictors| Overall
-------|-----------------------|-----------|-----------------------|-----------
      `1|ManufacturingProcess32 |4.0148629 | ManufacturingProcess32 |100.00000`
      2|ManufacturingProcess33 |2.5534232 | ManufacturingProcess13 |97.83640
      3|ManufacturingProcess28 |2.2725763 | BiologicalMaterial06   |82.21744
      4|ManufacturingProcess37 |2.1891986 | ManufacturingProcess17 |77.26777
      5|ManufacturingProcess13 |2.0341703 | BiologicalMaterial03   |76.21094
      6|ManufacturingProcess07 |1.7731421 | ManufacturingProcess36 |70.96498
      7|BiologicalMaterial05   |1.6380685 | BiologicalMaterial02   |68.78876
      8|ManufacturingProcess04 |1.6338725 | ManufacturingProcess09 |67.86384
      9|ManufacturingProcess16 |1.5879600 | BiologicalMaterial12   |63.36203
     10|BiologicalMaterial11   |1.5688048 | ManufacturingProcess06 |55.15443





```{r}
plot(caret::varImp(nnetChemTune), top = 10)

caret::varImp(nnetChemTune)

```


<br>

#### (c) Explore the relationships between the top predictors and the response for the predictors that are unique to the optimal nonlinear regression model. Do these plots reveal intuition about the biological or process predictors and their relationship with yield?


```{r}
library(gridExtra)

top9nnetPred <- trainTransformed %>%
   select(Yield, 
          ManufacturingProcess13,
          BiologicalMaterial06,
          ManufacturingProcess17,
          BiologicalMaterial03,
          ManufacturingProcess36,
          BiologicalMaterial02,
          ManufacturingProcess09,
          BiologicalMaterial12,
          ManufacturingProcess06)

plt01 <- top9nnetPred %>%
  ggplot(aes(x = ManufacturingProcess13, y = Yield)) +
  geom_point() + geom_smooth(method = "lm") + theme_bw()

plt02 <- top9nnetPred %>%
  ggplot(aes(x = BiologicalMaterial06, y = Yield)) +
  geom_point()+ geom_smooth(method = "lm") + theme_bw()

plt03 <- top9nnetPred %>%
  ggplot(aes(x = ManufacturingProcess17, y = Yield)) +
  geom_point() + geom_smooth(method = "lm") + theme_bw()

plt04 <- top9nnetPred %>%
  ggplot(aes(x = BiologicalMaterial03, y = Yield)) +
  geom_point() + geom_smooth(method = "lm") + theme_bw()

plt05 <- top9nnetPred %>%
  ggplot(aes(x = ManufacturingProcess36, y = Yield)) +
  geom_point() + theme_bw()

plt06 <- top9nnetPred %>%
  ggplot(aes(x = BiologicalMaterial02, y = Yield)) +
  geom_point() + geom_smooth(method = "lm") + theme_bw()

plt07 <- top9nnetPred %>%
  ggplot(aes(x = ManufacturingProcess09, y = Yield)) +
  geom_point() + geom_smooth(method = "lm") + theme_bw()

plt08 <- top9nnetPred %>%
  ggplot(aes(x = BiologicalMaterial12, y = Yield)) +
  geom_point() + geom_smooth(method = "lm") + theme_bw()

plt09 <- top9nnetPred %>%
  ggplot(aes(x = ManufacturingProcess06, y = Yield)) +
  geom_point() + geom_smooth(method = "lm") + theme_bw()

# Biological Predictors
grid.arrange(plt02, plt04, plt06, plt08, nrow = 2, top = "BIOLOGICAL PREDICTORS")

# Process Predictors
grid.arrange(plt01, plt03, plt05, plt07, plt09,
             nrow = 2, top = "PROCESS PREDICTORS")

```

<br>
<br>

We use scatter plots between the top predictors and the response for the TOP 9 predictors that are unique to the optimal nonlinear regression model.

The conclusion is:

* All four Biological predictors (`BiologicalMaterial02`, `BiologicalMaterial36`, `BiologicalMaterial06`, `BiologicalMaterial12`) appear to have low positive correlation to the response variable.

* The `ManufacturingProcess13` and `ManufacturingProcess17` appear to have low negative correlation to the response within a limited interval in each case. Also, there are a few outliers outside of the intervals.

* Predictor `ManufacturingProcess17` appears to have no correlation to the response. The points seem to form parallel vertical lines every 2.5 units. I am not sure why this predictor would come up in the top 10 predictors for the optimal non-linear regression model.

* Process predictors `ManufacturingProcess09` and `ManufacturingProcess06` seem to have a low positive correlation within a limited interval in each case.





